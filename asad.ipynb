{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUyh8COdopYj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHIXrYBhp4Bl"
      },
      "source": [
        "DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiF5eWNDtB-V",
        "outputId": "be57d29b-3bc5-4527-cdea-2c75dc2f3b09"
      },
      "outputs": [],
      "source": [
        "!pip install -q together openai groq regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dhw3f3C88s8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import regex\n",
        "from together import Together\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWcfo9cFqBsl"
      },
      "source": [
        "API KEYs & CLIENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewjmsuPQ9HrT"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOGETHER_API_KEY\"] = \"PUT YOUR API KEY HERE\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"PUT YOUR API KEY HERE\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"PUT YOUR API KEY HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hICjbFM5c5ji"
      },
      "outputs": [],
      "source": [
        "tg_client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
        "gq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "op_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkS-2oGi9upu"
      },
      "source": [
        "CALL LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y_YaGfbtFC7"
      },
      "outputs": [],
      "source": [
        "provider=\"together\"\n",
        "\n",
        "def call_llm (question : str):\n",
        "  if provider == \"together\":\n",
        "    response = tg_client.chat.completions.create(\n",
        "      model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": question\n",
        "          }\n",
        "      ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "  elif provider == \"groq\":\n",
        "    response = gq_client.chat.completions.create(\n",
        "        model=\"groq/compound-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\":question\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return(response.choices[0].message.content)\n",
        "  else:\n",
        "    response = op_client.responses.create(\n",
        "        model=\"gpt-5\",\n",
        "        input=question,\n",
        "        reasoning={ \"effort\": \"low\" },\n",
        "        text={ \"verbosity\": \"low\" },\n",
        "    )\n",
        "\n",
        "    return(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwykesd79l56"
      },
      "source": [
        "OUTPUT FORMATTING HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tmmpE990i_X"
      },
      "outputs": [],
      "source": [
        "def cleanCODE(text):\n",
        "    # Split the text into lines\n",
        "    lines = text.strip().split('\\n')\n",
        "\n",
        "    if lines and lines[0].startswith('```'):\n",
        "        lines = lines[1:]\n",
        "\n",
        "    closing_index = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if '```' in line:\n",
        "            closing_index = i\n",
        "            break\n",
        "\n",
        "    if closing_index is not None:\n",
        "        lines = lines[:closing_index]\n",
        "    clean_text = '\\n'.join(lines)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "def cleanJSON(text):\n",
        "    matchy = regex.search(r'\\{(?:[^{}]|(?R))*\\}', text)\n",
        "    return(matchy.group())\n",
        "\n",
        "\n",
        "def safe_json_parse(response: str, fallback: Any = None) -> Any:\n",
        "    try:\n",
        "        clean_response = cleanJSON(response)\n",
        "        return json.loads(clean_response)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fix invalid escape sequences by double-escaping\n",
        "        clean_response = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', clean_response)\n",
        "        try:\n",
        "            return json.loads(clean_response)\n",
        "        except json.JSONDecodeError:\n",
        "            return fallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qze0b-ZJ9iXS"
      },
      "source": [
        "## MAIN AGENT FUNCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiQb2_8poNK"
      },
      "source": [
        "TASK ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1h7n9lEPxT-"
      },
      "outputs": [],
      "source": [
        "def analyze_problem(code: str) -> Dict[str, Any]:\n",
        "    prompt =f\"\"\"\n",
        "    Role: You are the Main Analysis Agent. Your responsibility\n",
        "    is to perform a systematic static analysis of the given\n",
        "    code and identify defects that prevent correct execution.\n",
        "\n",
        "    Input: Buggy code: {code}\n",
        "\n",
        "    Instructions:\n",
        "    1. Assess the complexity of the debugging task and classify\n",
        "    it as SIMPLE or COMPLEX based on the following criterias:\n",
        "      - Number of critical bugs\n",
        "      - Degree of bug isolation\n",
        "      - Clarity of control flow\n",
        "      - Concurrency or resource management issues\n",
        "      - Inter-function or module coupling\n",
        "    2. Identify and precisely locate all bugs that may prevent\n",
        "    the code from executing correctly. For each bug, specify:\n",
        "      - Bug type (e.g., syntax error, API misuse, ...)\n",
        "      - Location (function name, line number range)\n",
        "      - Brief explanation of why it causes failure\n",
        "    3. Produce a structured, step-by-step repair plan\n",
        "    describing how the identified bugs should be fixed.\n",
        "\n",
        "    Output:Return a JSON object with the following schema:\n",
        "    {{\n",
        "      complexity\": \"SIMPLE\" or \"COMPLEX\",\n",
        "      \"bugs\": [\n",
        "        {{\n",
        "          \"type\": \"...\",\n",
        "          \"location\": \"...\",\n",
        "          \"explanation\": \"...\"\n",
        "        }}\n",
        "      ],\n",
        "      \"plan\": \"...\"\n",
        "    }}\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"complexity\": \"SIMPLE\", \"bugs\":\"...\", \"plan\": \"...\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aVqYnAKqLaB"
      },
      "source": [
        "NEW ITERATION ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtBY2gNf-CX7"
      },
      "outputs": [],
      "source": [
        "def new_iteration_analyze_problem(code: str, failure_log: Dict[str, Any], previous_plan: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    prompt =f\"\"\"\n",
        "    Role:\n",
        "        You are the Main Analysis Agent. Your responsibility is to perform a systematic review\n",
        "        of the given code after a previous fix attempt has failed. Identify all remaining defects\n",
        "        that prevent execution and generate a precise new repair plan. Do not provide suggestions\n",
        "        or potential improvements.\n",
        "\n",
        "    Input:\n",
        "        Previous repair plan: {previous_plan}\n",
        "        Failure summary: {failure_log[\"summary\"]}\n",
        "        Buggy code: {code}\n",
        "\n",
        "    Instructions:\n",
        "    1. Assess the complexity of the remaining debugging task and classify it as SIMPLE or COMPLEX\n",
        "      using the following criteria:\n",
        "          - Number of critical bugs remaining\n",
        "          - Degree of bug isolation\n",
        "          - Clarity of control flow\n",
        "          - Presence of concurrency or resource management issues\n",
        "          - Dependencies between functions or modules\n",
        "    2. Identify all remaining bugs, specifying for each:\n",
        "          - Bug type (e.g., syntax error, API misuse, logic error)\n",
        "          - Location (function name, line number range)\n",
        "          - Explanation of why it prevents correct execution\n",
        "    3. Produce a step-by-step new repair plan detailing how to fix the remaining bugs.\n",
        "\n",
        "    Output:\n",
        "    Return a JSON object with this schema:\n",
        "    {{\n",
        "      \"complexity\": \"SIMPLE\" or \"COMPLEX\",\n",
        "      \"bugs\": [\n",
        "        {{\n",
        "          \"type\": \"...\",\n",
        "          \"location\": \"...\",\n",
        "          \"explanation\": \"...\"\n",
        "        }}\n",
        "      ],\n",
        "      \"plan\":\"...\"\n",
        "    }}\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"complexity\": \"SIMPLE\", \"bugs\":\"...\", \"plan\": \"...\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XkIGUITqSMI"
      },
      "source": [
        "AGENTS CREATION AND PRIORITIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcluLVY1TCfQ"
      },
      "outputs": [],
      "source": [
        "def generate_agents(bugs: List[Any], plan: str) -> List[Dict[str, Any]]:\n",
        "    prompt = f\"\"\"\n",
        "    Role:\n",
        "        You are the Main Agent responsible for creating and organizing specialized agent profiles.\n",
        "        Your task is twofold:\n",
        "        1. Generate the minimum set of agents required to fix the located bugs.\n",
        "        2. Prioritize these agents based on their dependencies to ensure correct execution order.\n",
        "\n",
        "    Input:\n",
        "        Located bugs: {bugs}\n",
        "        Repair instructions: {plan}\n",
        "\n",
        "    Instructions:\n",
        "\n",
        "    Step 1 - Agent Generation:\n",
        "        1. Generate the minimum set of agents required to fix the bugs.\n",
        "        2. For each agent, provide:\n",
        "            - name\n",
        "            - role\n",
        "            - task_description (phrased as \"Your task is to...\" and explicitly referencing the located errors)\n",
        "\n",
        "    Step 2 - Agent Prioritization:\n",
        "        1. Determine dependencies between agents (e.g., syntax must be fixed before logic errors).\n",
        "        2. Order the agents based on these dependencies.\n",
        "\n",
        "    Output:\n",
        "    Return a JSON object with the following schema:\n",
        "\n",
        "    {{\n",
        "      \"agents\": [\n",
        "        {{\n",
        "          \"name\": \"...\",\n",
        "          \"role\": \"...\",\n",
        "          \"task_description\": \"...\"\n",
        "        }}\n",
        "      ],\n",
        "      \"execution_order\": [\"Agent_1_name\", \"Agent_2_name\", \"...\"]\n",
        "    }}\n",
        "\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"agents\": [], \"execution_order\": []})\n",
        "    return parsed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN3-bYmPqXbZ"
      },
      "source": [
        "ITERATIVE TASK REVIEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v3KeoHJyboO"
      },
      "outputs": [],
      "source": [
        "def task_review(agent: Dict[str, Any], agent_report: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "    Role:\n",
        "    You are the Main Agent responsible of reviewing the output of a specialized debugging agent.\n",
        "    Your task is to make the decision whether to approve or refine the agent fixes.\n",
        "\n",
        "    Input:\n",
        "    The agent task description,\n",
        "    {agent[\"task_description\"]}\n",
        "    The proposed fixed code\n",
        "    {agent_report[\"fixed_code\"]}\n",
        "    The fix explanation:\n",
        "    {agent_report[\"fix_explanation\"]}\n",
        "\n",
        "    Instructions:\n",
        "    1. Check whether the agent\\'s changes are correct and complete.\n",
        "    2. If yes, approve and move to the next agent.\n",
        "    3. If no, instruct the same agent to refine its work.\n",
        "\n",
        "    Your output in JSON format in the following fromat:\n",
        "    {{\n",
        "      \"decision\": \"APPROVE\" or \"REFINE\",\n",
        "      \"feedback\": \"...\" // if refinement is needed\n",
        "    }}\n",
        "\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"decision\": \"\", \"feedback\":\"\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seTJcsB7qbCp"
      },
      "source": [
        "FINAL VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2w9FYJjxcts"
      },
      "outputs": [],
      "source": [
        "def validate_solution(code: str) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "    Role:\n",
        "        You are the Master Agent responsible for final validation and closure.\n",
        "        Your task is to verify whether the given code is fully fixed and executable.\n",
        "        Focus only on bugs that prevent correct execution.\n",
        "\n",
        "    Input:\n",
        "        Buggy code: {code}\n",
        "\n",
        "    Instructions:\n",
        "    1. Check whether the code is free of execution-blocking bugs.\n",
        "    2. If the code is fully fixed, confirm completion.\n",
        "    3. If not, summarize and explain all remaining bugs.\n",
        "\n",
        "    Output:\n",
        "    Return a JSON object with this schema:\n",
        "\n",
        "    {{\n",
        "      \"status\": \"FIXED\" or \"NOT FIXED\",\n",
        "      \"summary\": \"...\",\n",
        "      \"remaining_bugs\": [\n",
        "        {{\n",
        "          \"type\": \"...\",\n",
        "          \"location\": \"...\",\n",
        "          \"explanation\": \"...\"\n",
        "        }}\n",
        "      ],\n",
        "      \"explanation\": \"...\"\n",
        "    }}\n",
        "\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"status\": \"NOT FIXED\", \"summary\": \"Parsing failed.\", \"remaining_bugs\": [], \"explanation\":\"\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkqTA6AZqeED"
      },
      "source": [
        "SIMPLE BUGS FAST FIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vRD8h3yw3mU"
      },
      "outputs": [],
      "source": [
        "def simple_fix(bugs: List[Any], code: str) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "    Role:\n",
        "        You are a Code Repair Expert. Your task is to fix all identified bugs in the given code\n",
        "        and produce a fully corrected version.\n",
        "\n",
        "    Input:\n",
        "        Buggy code: {code}\n",
        "        Summary of issues: {bugs}\n",
        "\n",
        "    Instructions:\n",
        "    1. Fix the code to eliminate all execution-blocking bugs.\n",
        "    2. Provide a clear explanation of each fix applied.\n",
        "\n",
        "    Output:\n",
        "    Return a JSON object with all newlines and quotes escaped (e.g., \\\\n, \\\\\"):\n",
        "\n",
        "    {{\n",
        "      \"fixed_code\": \"...\",\n",
        "      \"fix_explanation\": \"...\"\n",
        "    }}\n",
        "\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"fixed_code\": \"\", \"fix_explanation\":\"\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxB7urV9ax-"
      },
      "source": [
        "## SPECIALIZED AGENT TASK EXECUTION\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOrJBQyWqoTf"
      },
      "source": [
        "EXECUTE TASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjG-qaU4sbLV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def execute_agent(agent: Dict[str, Any], code: str) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "    Role:\n",
        "        You are a {agent[\"role\"]}.\n",
        "        Your task is to {agent[\"task_description\"]}.\n",
        "\n",
        "    Input:\n",
        "        Buggy code: {code}\n",
        "\n",
        "    Instructions:\n",
        "    1. Fix the code to address the issues within your responsibility.\n",
        "    2. Provide a clear explanation of the fix applied.\n",
        "\n",
        "    Output:\n",
        "    Return a JSON object with all newlines and quotes escaped (e.g., \\\\n, \\\\\"):\n",
        "\n",
        "    {{\n",
        "      \"fixed_code\": \"...\",\n",
        "      \"fix_explanation\": \"...\"\n",
        "    }}\n",
        "\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"fixed_code\": \"\", \"fix_explanation\":\"\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyw-RQ7DqrBV"
      },
      "source": [
        "REPEAT TASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoHWyHTq1ZOR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retry_execute_agent(agent: Dict[str, Any], code: str, feedback: str) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "    Role:\n",
        "        You are a {agent[\"role\"]}.\n",
        "        Your task is to {agent[\"task_description\"]}.\n",
        "        You previously attempted a fix that failed. Now, fix the code again using the provided feedback.\n",
        "\n",
        "    Input:\n",
        "        Buggy code: {code}\n",
        "        Feedback: {feedback}\n",
        "\n",
        "    Instructions:\n",
        "    1. Fix the code, taking the feedback into account.\n",
        "    2. Provide a clear explanation of the fix applied.\n",
        "\n",
        "    Output:\n",
        "    Return a JSON object with all newlines and quotes escaped (e.g., \\\\n, \\\\\"):\n",
        "\n",
        "    {{\n",
        "      \"fixed_code\": \"...\",\n",
        "      \"fix_explanation\": \"...\"\n",
        "    }}\n",
        "\n",
        "    Do not write any text besides the JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    result = call_llm(prompt)\n",
        "    parsed = safe_json_parse(result, fallback={\"fixed_code\": \"\", \"fix_explanation\":\"\"})\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aThXGg9_-DLL"
      },
      "source": [
        "## MAIN EXECUTION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1SOM1MHA8Fz"
      },
      "outputs": [],
      "source": [
        "def adaptive_debugger(buggy_code: str, max_iterations: int = 5):\n",
        "    analysis_report = analyze_problem(buggy_code)\n",
        "    print(analysis_report)\n",
        "    if not analysis_report[\"bugs\"]:\n",
        "        print(\"\\nðŸŸ¢ No errors found. Code is already fixed\")\n",
        "        return buggy_code\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "\n",
        "        print(f\"\\n--------------ITERATION {iteration + 1}\")\n",
        "\n",
        "        if analysis_report[\"complexity\"] == \"SIMPLE\":\n",
        "            print(\"\\n--------------USING SIMPLE FIX\")\n",
        "            code_to_debug=simple_fix(analysis_report[\"bugs\"], buggy_code)\n",
        "\n",
        "        else:\n",
        "            print(\"\\n--------------USING MULTI AGENTS\")\n",
        "\n",
        "            created_agents= generate_agents(analysis_report['bugs'], analysis_report['plan'])\n",
        "            agent_profiles= created_agents[\"agents\"]\n",
        "            execution_order = created_agents[\"execution_order\"]\n",
        "\n",
        "\n",
        "            print(\"\\nNumber of Created Agents:\", len(agent_profiles))\n",
        "            print(\"\\nExecution Order: \" +\" -> \".join(execution_order))\n",
        "            reports = []\n",
        "            code_to_debug = buggy_code\n",
        "            for agent_name in execution_order:\n",
        "                print(f\"\\nExecuting Agent: {agent_name}\")\n",
        "                agent = next((a for a in agent_profiles if a[\"name\"] == agent_name), None)\n",
        "                if not agent:\n",
        "                    continue\n",
        "                decision=\"\"\n",
        "                feedback=\"\"\n",
        "                review_iterations=0\n",
        "                while not(decision == \"APPROVE\") and review_iterations < 3:\n",
        "\n",
        "                    if decision==\"REFINE\":\n",
        "                        print(f\"\\nAgent: {agent_name} needs refinement. Retrying\")\n",
        "                        result = retry_execute_agent(agent, code_to_debug, feedback)\n",
        "                    else:\n",
        "                        result = execute_agent(agent, code_to_debug)\n",
        "\n",
        "                    review = task_review(agent, result)\n",
        "                    decision = review[\"decision\"]\n",
        "                    feedback = review[\"feedback\"]\n",
        "\n",
        "                    if decision=='APPROVE':\n",
        "                        print(f\"\\nAgent: {agent_name} approved\")\n",
        "                        print(f\"\\nAgent: {agent_name} execution completed\")\n",
        "\n",
        "                    code_to_debug = result[\"fixed_code\"]\n",
        "\n",
        "\n",
        "                    review_iterations+=1\n",
        "\n",
        "\n",
        "        validation = validate_solution(code_to_debug)\n",
        "        status = validation[\"status\"]\n",
        "        summary = validation[\"summary\"]\n",
        "        re_bugs = validation[\"remaining_bugs\"]\n",
        "        if status == \"FIXED\":\n",
        "            print(f\"\\nðŸŸ¢ Bugs fixed successfully. Total number of iterations: {iteration + 1}\\n\")\n",
        "            return code_to_debug\n",
        "\n",
        "        print(\"\\nðŸŸ¡ Failed to fix the bugs. Retrying....\")\n",
        "        print(f\"\\nValidation report : \\n {summary} \\n {re_bugs}\")\n",
        "        prev_analysis_report = analysis_report\n",
        "        analysis_report = new_iteration_analyze_problem(code_to_debug, validation, analysis_report)\n",
        "\n",
        "    print(\"\\nðŸ”´ Failed to fix the bugs after max iterations.\")\n",
        "    return code_to_debug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaaUr4OLU5Nr"
      },
      "source": [
        "## BUGGY CODE EXAMPLE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4tzqzPZfvOG"
      },
      "outputs": [],
      "source": [
        "example_buggy_code=\"\"\"\n",
        "def process_student_records(students, output_file):\n",
        "\n",
        "    averages = {}\n",
        "    for student in students:\n",
        "        grades = student[\"grades\"]\n",
        "        avg = sum(grades) / (len(grades) - 1)\n",
        "        averages[student[\"name\"]] = avg\n",
        "\n",
        "    top_student = max(averages, key=lambda k: averages[k])\n",
        "\n",
        "    normalized = []\n",
        "    for g in grades:\n",
        "        normalized.append(g / max(grades))\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(\"Student Averages:\\n\")\n",
        "        for name, avg in averages.items():\n",
        "            f.write(f\"{name}: {avg}\\n\")\n",
        "\n",
        "        f.write(f\"\\nTop student: {top_student} with {averages[top_student]}\\n\")\n",
        "\n",
        "    summary = \"Processed {count} students\".format(cnt=len(students))\n",
        "    f.write(summary)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    import jsonn\n",
        "    result = json.dumps({\n",
        "        \"averages\": averages,\n",
        "        \"top_student\": top_student\n",
        "    })\n",
        "\n",
        "    return result\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic21bb2Dr-LU"
      },
      "source": [
        "## EXECUTION EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "Nt5_WFbQnNh8",
        "outputId": "e8b93f19-c8d6-435f-b223-f72c138f7d70"
      },
      "outputs": [],
      "source": [
        "fixed_code = adaptive_debugger(example_buggy_code)\n",
        "print(\"Final Output:\\n\", fixed_code)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
